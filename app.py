import streamlit as st
import requests
import tempfile

# -------------------------
# CONFIG: Put your keys here
# -------------------------
AZURE_SPEECH_KEY = "FS4yBV3YjzD9gw2g8Xzcz1k8OVpIXR8QaB0NuZt5ODQmappDVzirJQQJ99BKAC3pKaRXJ3w3AAAYACOGhPZt"
AZURE_SPEECH_REGION = "eastasia"  # just the region

OPENROUTER_API_KEY = "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"
OPENROUTER_MODEL = "google/gemini-3-pro-preview"

# -------------------------
# Streamlit page config
# -------------------------
st.set_page_config(page_title="Speech ‚Üí OpenRouter ‚Üí Speech", layout="centered")
st.title("üé§ Speech ‚Üí OpenRouter ‚Üí Speech")

# -------------------------
# Session state defaults
# -------------------------
if "audio_bytes" not in st.session_state:
    st.session_state["audio_bytes"] = None
if "transcript" not in st.session_state:
    st.session_state["transcript"] = ""
if "llm_answer" not in st.session_state:
    st.session_state["llm_answer"] = ""
if "tts_audio" not in st.session_state:
    st.session_state["tts_audio"] = None

# -------------------------
# 1Ô∏è‚É£ Record audio in browser
# -------------------------
audio_bytes = st.audio_input("Click to speak")
if audio_bytes:
    st.session_state["audio_bytes"] = audio_bytes
    st.success("Audio recorded!")

# -------------------------
# Helper: Save audio temporarily
# -------------------------
def save_temp_audio(audio):
    tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    tmp_file.write(audio.read())
    tmp_file.flush()
    tmp_file.close()
    return tmp_file.name

# -------------------------
# 2Ô∏è‚É£ Speech-to-Text (Azure REST)
# -------------------------
def azure_speech_to_text(audio_path):
    url = f"https://{AZURE_SPEECH_REGION}.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1?language=en-US"
    headers = {
        "Ocp-Apim-Subscription-Key": AZURE_SPEECH_KEY,
        "Content-Type": "audio/wav"
    }
    with open(audio_path, "rb") as f:
        data = f.read()
    try:
        response = requests.post(url, headers=headers, data=data, timeout=15)
        if response.status_code == 200:
            return response.json().get("DisplayText", "")
        else:
            st.error(f"STT error: {response.status_code} {response.text}")
            return ""
    except requests.exceptions.RequestException as e:
        st.error(f"STT request failed: {str(e)}")
        return ""

# -------------------------
# 3Ô∏è‚É£ Send text to OpenRouter LLM
# -------------------------
def ask_openrouter(prompt_text):
    url = "https://openrouter.ai/api/v1"
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": OPENROUTER_MODEL,
        "messages": [{"role": "user", "content": prompt_text}]
    }

    try:
        response = requests.post(url, headers=headers, json=payload, timeout=20)

        # Debug print (optional)
        # st.write("RAW:", response.text)

        if response.status_code == 200:
            return response.json()["choices"][0]["message"]["content"]

        elif response.status_code == 401:
            st.error("‚ùå Invalid API key (401 Unauthorized)")
            return ""

        elif response.status_code == 404:
            st.error("‚ùå Model not found (404 Not Found)")
            return ""

        else:
            st.error(f"LLM error {response.status_code}: {response.text}")
            return ""

    except requests.exceptions.RequestException as e:
        st.error(f"LLM request failed: {str(e)}")
        return ""


# -------------------------
# 4Ô∏è‚É£ Text-to-Speech (Azure REST)
# -------------------------
def azure_text_to_speech(text):
    url = f"https://{AZURE_SPEECH_REGION}.tts.speech.microsoft.com/cognitiveservices/v1"
    headers = {
        "Ocp-Apim-Subscription-Key": AZURE_SPEECH_KEY,
        "Content-Type": "application/ssml+xml",
        "X-Microsoft-OutputFormat": "riff-16khz-16bit-mono-pcm"
    }
    ssml = f"""
    <speak version='1.0' xml:lang='en-US'>
        <voice xml:lang='en-US' xml:gender='Female' name='en-US-JennyNeural'>
            {text}
        </voice>
    </speak>
    """
    try:
        response = requests.post(url, headers=headers, data=ssml.encode("utf-8"), timeout=15)
        if response.status_code == 200:
            return response.content
        else:
            st.error(f"TTS error: {response.status_code} {response.text}")
            return None
    except requests.exceptions.RequestException as e:
        st.error(f"TTS request failed: {str(e)}")
        return None

# -------------------------
# SINGLE BUTTON: Record ‚Üí LLM ‚Üí Speak
# -------------------------
if st.button("Record ‚Üí Ask ‚Üí Speak"):
    if st.session_state["audio_bytes"] is None:
        st.warning("Record audio first!")
    else:
        tmp_path = save_temp_audio(st.session_state["audio_bytes"])
        # 1Ô∏è‚É£ STT
        st.session_state["transcript"] = azure_speech_to_text(tmp_path)
        if st.session_state["transcript"].strip() == "":
            st.error("No transcription available.")
        else:
            # 2Ô∏è‚É£ LLM
            st.session_state["llm_answer"] = ask_openrouter(st.session_state["transcript"])
            # 3Ô∏è‚É£ TTS
            if st.session_state["llm_answer"].strip() != "":
                st.session_state["tts_audio"] = azure_text_to_speech(st.session_state["llm_answer"])

# -------------------------
# DISPLAY
# -------------------------
st.subheader("Transcribed Text")
st.text_area("Transcript", st.session_state["transcript"], height=150)

st.subheader("LLM Answer")
st.text_area("LLM Answer", st.session_state["llm_answer"], height=150)

if st.session_state["tts_audio"]:
    st.audio(st.session_state["tts_audio"], format="audio/wav")
